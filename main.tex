\documentclass{article}

\usepackage[final]{nips_2018}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{amsmath}
\usepackage{mathtools}

\usepackage{subfiles}

\usepackage{graphicx}

\title{CECS 551 Project}

\author{%
  Jared R.~Coleman\\
  Computer Engineering and Computer Science\\
  California State University, Long Beach\\
  Long Beach, CA 90802 \\
  \texttt{jared.coleman@student.csulb.edu} \\
  \And 
  Taina G.D.~Coleman\\
  Computer Engineering and Computer Science\\
  California State University, Long Beach\\
  Long Beach, CA 90802 \\
  \texttt{taina.coleman@student.csulb.edu} \\
  \And 
  Ian M. ~Schenck\\
  Computer Engineering and Computer Science\\
  California State University, Long Beach\\
  Long Beach, CA 90802 \\
  \texttt{ian.schenck@student.csulb.edu} \\
}

\begin{document}
\stepcounter{equation}
\setcounter{equation}{0}

\maketitle

\begin{abstract}
   Since its introduction in 2014, the Generative Adverserial Network (GAN) has been adapted and improved upon at Machine Learning conferences all over the world. In this short survey, we discuss the fundamentals of GAN and one of its successors, the Wasserstein GAN (WGAN). In the second half of our report, we discuss the application of convolutional neural networks on graphs through the Graph Convolutional Network (GCN), which has shown to be a computationally efficient and accurate method of node classification via semi-supervised learning.
\end{abstract}

\subfile{introduction}
\subfile{gan}
\subfile{wgan}
\subfile{gcn}

\section{Conclusion \& Future Work}
  \subsection{Generative Adversarial Networks}
  \subsection{Graph Convolutional Networks}
  The GCN model is shown to be more effective than other proposed methods of deep learning on graph data, but it is not without its flaws. The memory requirement is $\mathcal{O}(\mathcal{E})$, growing linearly with the dataset, so very large datasets can cause problems. Also, this model does not function well with more than a few layers. One paper that improves upon the GCN focuses on further understanding GCN while proposing a combined co-training and self-training method to improve classification accuracy without requiring additional labels in the training set~\cite{Li2018}. Another paper proposes adaptive layer-wise sampling to accelerate training and reduce memory requirements~\cite{Huang2018}. A third paper we want to research develops a new method of applying convolutional operations on graphs called learnable graph convolutional layers, which allows for deeper networks and yields higher classification accuracy with less memory usage than the GCN~\cite{Gao2018}. For the next part of the semester we plan on implementing a Graph Convolutional Network, hopefully utilizing some of the improvements developed above.
  

\bibliographystyle{plain}
\bibliography{references}

\end{document}

