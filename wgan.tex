
As mentioned in the past section, training GANs is usually very slow and unstable, so the Wasserstein Adversarial Network - WGAN was introduced in ~\cite{Arjovsky2017} as an improvement in order to make training less difficult. The layout of the WGAN is different than the regular GAN. Now we only have one neural network in the model, the Generator, and a system that the author of the WGAN paper calls the critic. Instead of using another Neural Network to classify the data as real or fake, WGAN uses the critic to find the distance function in between a real data probability distribution and a fake one, and then uses this function to calculate the loss, and finally update the weights to start a new iteration.

When training GAN models the goal is to find a distribution $P_{\theta}$ that is the most similar to a real distribution $P_{r}$, where $\theta$ is the parameters. There are two approaches to perform this task:
1. Define a parametric family of densities $P_{\theta}$, which is as a differentiable function $P_{\theta} >= 0$ and integral ($P_{\theta}$(x) dx) = 1, where x is a real data sample. Try to directly learn $P_{theta}$ by optimizing it through maximum likelihood estimation (MLE).
2. Define a variable Z with an existent distribution $p(z)$ and pass it through the Generator, $g_{\theta}$, that will generate samples that follow the distribution $P_{theta}$. By updating $\theta$ the $P_{theta}$ can be modified to get more similar to $P_{r}$.

The first approach usually runs into problems since trying to maximize MLE:
\begin{center}
	$max_{\theta\in \mathbb{R}^{d}} \frac{1}{m} \sum_{i=1}^{m} log P_{\theta}(x^{(i)})$
\end{center}

Is the same as trying to minimize KL divergence.


In KL divergence if Q(x) = 0, which in this case is $P_{\theta}$, for a $x$ where $P_{r}(x) > 0$, the result goes to $+\infty$. This makes improbable for $P_{r}$ to fall within that support in low dimensional manifolds, which means that if one point of $P_{\theta}$ lays outside of the support, KL will not be defined. In order to try to remediate this problem, a noise term called the Gaussian noise, with a large enough bandwidth to cover all the examples, is added to the distribution $P_{\theta}$. Although by applying this maneuver, error is being added to the system resulting in poor quality images. Also, it could be computationally expensive to sample from $P_{\theta}$ even if it is learned.

The latter approach has two big advantages: it can represent distributions in a low dimension manifold, and unlike the densities, it is easier to generate samples of the distribution. To get to the goal of finding a distribution  $P_{\theta}$ as close to $P_{r}$ as possible, the parameter $\theta$ needs to be optimized, in which the mapping $\theta \rightarrow P_{\theta}$ should be continuous, since the objective is when a sequence of parameters converge to $\theta$, the distribution correspondent to those parameters converges to $P_{\theta}$. To measure the similarity, or distance in between the distributions, in order to keep fine tuning $\theta$, the paper goes on to present various metrics that are commonly used. 

\subsection{The Distances}
The metrics presented were:
\begin{itemize}
	\item Total Variation (TV) distance:
	\begin{center}
		$\delta(P_{r}, P_{g}) = sup_{\mathbb{A \in \sum}}|P_{r}(A)-P_{g}(A)| $
	\end{center}
	\item Kullback-Leibler (KL) divergence:
	\begin{center}
		$KL(P_{r}, P_{g}) = \int log \Big(\frac{P_{r}(x)}{P_{g}(x)}\Big) P_{r}(x) d\mu(x) $
	\end{center}
	\item Jenson-Shannon divergence:
	\begin{center}
		$ JS(P_{r}, P_{g}) = KL(P_{r} \Vert P_{g}) + KL(P_{g} \Vert P_{r}) $
	\end{center}
	\item Earth Mover (EM) or Wasserstein-1 distance:
	\begin{center}
		$ W(P_{r}, P_{g}) = inf_{\gamma \in \Pi(P_{r},P_{g})} \mathbb{E}_{(x,y)~\gamma}[\Vert x - y \Vert]$
	\end{center}
	
\end{itemize}

where $\Pi(P_{r}, P_{g})$ is the set of all joint distributions $\gamma(x,y)$ whose marginals are $P_{r}$ and $P_{g}$ and $\gamma$ intuitively represents the "mass" that must be moved from x to y in order to transform the distribution  $P_{g}$ into $P_{r}$. So W is the "cost" of the optimal transport.

The paper then shows an example where the only metric defined for that problem is EM, showing the advantages of it over the other ones.

\subsection{The WGAN}
After showing some of EM's advantages, the author explains that one of the issues with EM is that the infimum is highly intractable. In order to use EM, the Kantorovich-Rubinstein duality is used to approximate it to:
\begin{center}
	$W(P_{r},P_{\theta}) = sup_{\Vert f \Vert_{L \leq 1}} \mathbb{E}_{x \sim P_{r}}[f(x)] - \mathbb{E}_{x \sim P_{\theta}}[f(x)] $
\end{center}



where the supremum is over all 1-Lipschitz functions f : X $\rightarrow$ R. Considering K-Lipschitz functions (instead of $f : \Vert f \Vert < 1, \Vert f \Vert < K$ ) for some constant K, then the supremum can be solved  for $K*W(P_{r}, P_{g})$, which is true because a K-Lipchitz function is a 1-Lipchitz function divided by K. Taking a parameterized family of  functions $f_{w}$, where w $\in$ W, w are the weights and W is the set of possible weights,  the supremum is still intractable, but can be approximated by:
\begin{center}
	$ max_{w \in W} \mathbb{E}_{x \sim P_{r}}[f_{w}(x)] - \mathbb{E}_{x \sim P_{\theta}}[f_{w}(x)] \leq sup_{\Vert f \Vert_{L \leq 1}} \mathbb{E}_{x \sim P_{r}}[f(x)] - \mathbb{E}_{x \sim P_{\theta}}[f(x)] = K . W(P_{r}, P_{\theta}) $
\end{center}

For optimization purposes there is no need to know the value of K since it is a constant fixed throughout the process and will get absorbed into the parameter tuning.

\subsubsection {Describing the training:}

The goal is to get $P_{theta}$ to match $P_{r}$. First, the optimal $f_{w}$ for the distance is calculated for a fixed value $g_{theta}$ of the generator. Then,  performing  backpropagation through $W(P_{r}, g_{theta}(z))$ and sampling several z, the gradient for $\theta$ is found. The final step is to update $\theta$ and repeat the process. The paper offers the algorithm for better understanding:


An observation to be made is that as observed in the algorithm, to guarantee the K-Lipchitz condition the weights have to be constrained in a [-c, c] region, such maneuver is called clipping. The author defends that this is not the best resolution for the problem, the topic is still being researched, but this trick is simple and showed good performance.

\subsubsection{Difference in between GAN and WGAN}
GAN is a model composed by two neural networks competing against each other, one generating fake data, the generator, and another one trying to classify which data is fake and which is real, the discriminator. This is usually presented as a min max problem, but in reality the discriminator is never trained to convergence and the updates are made towards some approximation of the JS divergence. On the other hand, WGAN is composed by a neural network, the generator, and a critic responsible to find an optimal  $f_{w}$ to get the best approximation of the distance $W(P_{r},P_{\theta})$. Since the Wasserstein distance is differentiable at all points, $f_{w}$ is trained to convergence before the weights update to achieve a better gradient result (minimized loss).   



\subsection{Empirical Experiments}
On the first experiment a DCGAN and a WGAN are trained using the LSUN-Bedrooms dataset with the goal of generating bedroom images. The results for WGAN were highly successful since the critic is very well trained, and the loss function is a very good estimate of the Wasserstein distance function. For the DCGAN, the JS saturates at a value approximately 0.69, the discriminator has zero loss, and the generated samples are sometimes good, but sometimes collapsed into a nonsense image. 

In conclusion, the author observations for the WGAN are that increases stability of learning, avoid problems like mode collapse and provide learning curves useful for debugging and hyperparameter searching. 