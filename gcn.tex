
\section{Graph Convolutional Networks}

\subsection{Background}
The success of deep learning paradigms such as convolutional neural networks (CNN) with Euclidean data (images, text and video) has led to research on how non-Euclidean data such as graphs can be effectively analyzed through deep learning. In a standard CNN, convolutions are relatively simple to compute, as input sizes are always uniform. Graphs make this more complicated, as they can have different numbers of unordered nodes, each with different numbers of neighbors~\cite{Wu2019}. Much of the research into deep learning with graphs is based on spectral graph theory, which is the analysis the properties and the structure of a graph from its spectrum, or set of eigenvalues and eigenvectors. In this overview we will focus on Kipf et al.'s research on semi-supervised classifications using Graph Convolutional Networks (GCN)~\cite{Kipf2016}, which uses a first-order approximation of the spectral filters on graphs developed in previous research on GCNs~\cite{Bruna2013}. GCNs based on spectral graph theory look at spectral convolutions as removing noise from a graph signal $x \in \mathbb{R}^n$ with a filter $g_\theta$,

\begin{equation}
\label{spectral_prop}
g_\theta \star x = Ug_\theta U^Tx
\end{equation}
$g_\theta$ is understood as a function of eigenvalues of the normalized graph laplacian, $L = I_N - D^{-\frac{1}{2}}AD^{-\frac{1}{2}} = U\Lambda U^T$ where $A$ is the graph's adjacency matrix, $D$ is a diagonal matrix of node degrees, $D_{ii} = \sum_{j} (A_{i,j})$, $U$ is a matrix of eigenvectors ordered by eigenvalues, and $\Lambda$ is is the diagonal matrix of eigenvalues. Multiplication with $U$ is $\mathcal{O}(N^2) $ and finding the eigenvectors and eigenvalues of $L$ can grow very expensive for large graphs. To reduce the computational cost of GCNs, a truncated expansion of Chebyshev polynomials could be used to approximate $g_\theta (\Lambda)$~\cite{Defferrard2016}.
Chebyshev polynomials are defined recursively as $T_k(x) = 2xT_{k-1}(x) - T_{k-2}(x)$ with $T_0(x) = 1$ and $T_1(x) = x$~\cite{Hammond2011}.
\begin{equation}
\label{cheby_prop}
g_{\theta'} \star x \approx U(\sum\limits_{k=0}^{K}\theta'_k T_k (\tilde{\Lambda}))U^Tx = \sum\limits_{k=0}^{K} \theta'_k T_k (\tilde{L})x
\end{equation}
This simplification can occur because $U \Lambda^k U^T = (U \Lambda U^T)^k = L^k $, so you can simplify the equation by using the Chebyshev polynomial of $\tilde{L} = \frac{2}{\lambda_{max}} L - I_n$. This approximation avoids any multiplication with $U$, significantly reducing the computation time. The propagation rule proposed in Kipf et al. further approximates Equation $(\ref{cheby_prop})$ by assuming $K = 1$ and $\lambda_{max} = 2$ (which makes $\tilde{L} = D^{-\frac{1}{2}}AD^{-\frac{1}{2}}$), reducing the computational cost even more and reduces overfitting on graphs with wide node degree distributions~\cite{Kipf2016}. These simplified values can be plugged in to Equation $(\ref{cheby_prop})$:
\begin{equation}
\label{reduce_k}
g_{\theta'} \star x \approx \sum\limits_{k=0}^{1} \theta'_k T_k (D^{-\frac{1}{2}}AD^{-\frac{1}{2}})x = \theta'_0 T_0(D^{-\frac{1}{2}}AD^{-\frac{1}{2}})x + \theta'_1 T_1(D^{-\frac{1}{2}}AD^{-\frac{1}{2}})x 
\end{equation}
Recalling the recursive definition of Chebyshev polynomials described above, this simplifies to:
\begin{equation}
\label{simplified_1stcheby}
g_{\theta'} \star x \approx \theta'_0x + \theta'_1 D^{-\frac{1}{2}}AD^{-\frac{1}{2}}x 
\end{equation}
To reduce overfitting and minimize operations per layer, they only used a single parameter $\theta = \theta'_0 = -\theta'_1$, allowing the equation to be factored in this form:
\begin{equation}
\label{single_param}
g_\theta \star x \approx \theta(I_N + D^{-\frac{1}{2}}AD^{-\frac{1}{2}})x
\end{equation}
One issue with this approximation is their decision to set $\lambda_{max} = 2$, meaning the eigenvalue range is $[0,2]$, which they found could cause numerical instability and exploding/vanish gradients in the GCN~\cite{Kipf2016}. They added self connections to the adjacency matrix, $\tilde{A} = A + I_N$, and used the diagonal matrix of the node degrees of $\tilde{A}$, replacing $I_N + D^{-\frac{1}{2}}AD^{-\frac{1}{2}}$ with $\hat{A} = \tilde{D}^{-\frac{1}{2}}\tilde{A}\tilde{D}^{-\frac{1}{2}}$. These additions bring us to the layer-wise propagation rule~\cite{Kipf2016}:
\begin{equation}
\label{prop_rule}
H^{(l+1)} = \sigma(\hat{A}H^{(l)}W^{(l)})
\end{equation}
$H^{(l)} \in \mathbb{R}^{N x C}$ is the input signal, where $H^{(0)} = X$, and $W^{(l)} \in \mathbb{R}^{C x F}$ is the filter parameter matrix for the current layer, $\sigma(\cdot)$ is an activation function, and $H^{(l+1)} \in \mathbb{R}^{N x F}$ is the convolved signal matrix~\cite{Kipf2016}. The renormalized adjacency matrix $\hat{A}$ can be represented as a sparse matrix, so multiplying $\hat{A}$ by $H^{(l)}$ only has a complexity of $\mathcal{O}(|\mathcal{E}|)$, giving the entire operation a complexity of $\mathcal{O}(|\mathcal{E}|FC)$.  This new graph convolution is localized in space, so each row of the output $Z$ contains a latent representation of each node of input $X$ as well as its neighbors, with values from $\hat{A}$ determining how much weight each neighbor is given in the latent representation.

\subsection{Model Architecture and Experiments}
With Equation \ref{prop_rule}, they build a deep learning model for semi-supervised node classification, where the goal is to classify all nodes in a graph where only a few have labels. They made a forward model for a two layer network using the propagation rule in Equation \ref{prop_rule}:
\begin{equation}
\label{forward_model}
Z = f(X, A) = softmax(\hat{A} ReLU(\hat{A}XW^{(0)})W^{(1)})
\end{equation}
In this model, $W^{(0)} \in \mathbb{R}^{C x H}$ and $W^{(1)} \in \mathbb{R}^{H x F}$, $H$ being number of hidden features, and $F$ being the number of output features.
In a supervised learning network, we could then calculate cross-entropy loss for all training examples, but in this semi-supervised learning network only a few examples have labels, so they evaluate loss by calculating cross-entropy error for each labeled example. Then $W^{(0)}$ and $W^{(1)}$ are trained via gradient descent.
To experiment with this model, they trained it on three citation networks, with 20 labeled examples per class, a dataset extracted from a knowledge graph with only one labeled example per class, and random, featureless graphs with dummy labels. For the citation networks, they trained each model for maximum $200$ epochs, using a dropout rate of $0.5$ prior to both convolutions, $16$ units in the hidden layer, the Adam optimizer, a learning rate of $0.01$, weight decay of $0.0005$, Xavier weight initialization, and an early stop tolerance of 10 epochs of stagnant validation loss before ending training~\cite{Kipf2016}. 

After training the models, they compared mean classification accuracy of 100 runs with six other models, and found that their model had higher accuracy than the rest for each dataset. After comparing to other models, they compared the classification accuracy of the final propagation model from Equation $(\ref{prop_rule})$ against the others discussed in the paper, and their model had superior accuracy compared to all the rest. Finally, they compared their two-layer models with models from one to ten layers deep, along with a variant that uses residual connections between layers by adds the hidden layer $H^{(l)}$ to the propagation model in Equation $\ref{prop_rule}$. They found that for the citation networks, 2 or 3 layer models gave the best results, with or without the previous layer's connections added. At deeper layers, overfitting became more of an issue, and the standard model's accuracy decreased drastically. The models with residuals did not decrease in accuracy as rapidly as those without, but they still had lower accuracy than the 2 or 3 layer models. 